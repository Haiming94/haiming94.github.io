### 朴素贝叶斯

朴素贝叶斯（Naive Bayes）是基于贝叶斯定理于特征条件独立假设的分类方法，属于统计学分类方法。朴素贝叶斯分类器假设在给定样本类别的条件下，样本的每个特征与其他特征均不相关，对于给定的输入，利用贝叶斯定理，求出后验概率最大的输出。


#### 1.基于贝叶斯的分类

​	基于概率的分类问题，就是要求出使得 $P(Y|X)$ 最大的 $Y$ 的取值。设 $X$ 是定义在 $n$ 维向量空间上的随机变量，$Y$ 是定义在输出空间 ${y_1, y_2, ..., y_k}$ 上的随机变量，训练数据集公有 $N$ 个样本：
$$
T={(x_1,y_1),(x_2,y_2),...,(x_N,y_N)}
$$
根据贝叶斯定理，给定样本 $x$ 的条件下，其类别取 $y_i$ 的概率为：
$$
P(Y=y_i|X=x)=\frac{P(X=x|Y=y_i)\cdot P(Y=y_i)}{\sum_j P(X=x|Y=y_j)P(Y=y_j)}
$$
条件概率分布 $P(X=x|Y=y_i)$ 称为**似然项**，假设 $x$ 的第 $i$ 项可能取值有 $S_i$ 个，$Y$ 可能的取值有 $K$ 个，那么要求得样本 $x$ 所属类别需要估计的参数总量为 $K(\prod _i^n S_i - 1)$（此处需要减1是因为条件概率的总和为1）。由此可见，直接使用贝叶斯推理的参数数量为指数级，在实际中是不可取的。

#### 2.朴素贝叶斯法

​	朴素贝叶斯法对条件概率分布做了独立性假设，极大减少了参数数量。朴素贝叶斯法假设样本的所有特征在给定所属类别的情况下相互独立，即：
$$
P(X=x|Y=y_i) =P(X^{(1)}=x^{(1)},...,X^{(n)}=x^{(n)}|Y=y_i) \\
 =\prod _{j=1}^n P(X^{(j)}=x^{(j)}|Y=y_i)
$$
其中，$x^(j)$ 为样本 $x$ 的第 $j$ 个特征。因此，基于朴素贝叶斯假设，后验概率为：
$$
P(Y=y_i|X=x)=\frac{P(Y=y_i) \prod _{j=1}^n P(X^{(j)}=x^{(j)}|Y=y_i)}{\sum _{k}P(Y=y_k)\prod_{j=1}^n P(X^{(j)}=x^{(j)}|Y=y_k)}
$$
​	朴素贝叶斯法学习样本的类条件概率，属于生成模型。朴素贝叶斯分类器将后验概率最大的类别作为样本的归属，因此，分类模型可表示为：
$$
y=\mathop{argmin}_{y_i} P(Y=y_i|X=x) \\
=\mathop{argmin}_{y_i} \frac{P(Y=y_i) \prod _{j=1}^n P(X^{(j)}=x^{(j)}|Y=y_i)}{\sum _{k}P(Y=y_k)\prod_{j=1}^n P(X^{(j)}=x^{(j)}|Y=y_k)}
$$
​	由于上式分母的取值不依赖于样本的类属 $y_k$ ，于是，朴素贝叶斯分类器可简化为：
$$
y=\mathop{argmin}_{y_i} P(Y=y_i) \prod _{j=1}^n P(X^{(j)}=x^{(j)}|Y=y_i)
$$
​	条件独立性假设是一个较强的假设，视朴素贝叶斯分类变得简单易于实现。

#### 3.朴素贝叶斯学习算法

[朴素贝叶斯分类器：例子解释](https://blog.csdn.net/xo3ylAF9kGs/article/details/78630937)

[机器学习：说说贝叶斯分类](https://mp.weixin.qq.com/s?__biz=MzI3NTkyMjA4NA==&mid=2247484265&idx=1&sn=b3f3e96677c7648579841b518e3c4dd1&chksm=eb7c2ea2dc0ba7b4a92049dce6ea3b72091c3358dc27f4736aea22ff4119e2257a0423f1987f&scene=21#wechat_redirect)
