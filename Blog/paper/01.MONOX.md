## 【阅读论文】Can Monolingual Pretrained Models Help Cross-Lingual Classification?

- 摘要

因为模型“容量”有限(the constant model capacity)，多语言预训练模型通常落后于单个语言。
文章提出了两种改进小样本(zero-shot)跨语言分类的方法，将当语言训练的模型转移给多语言。
两种方法在跨语言分类的基准任务上实验，比普通的多语言微调效果有效。

- introduction

有监督的文本分类严重依赖于手工注释的训练数据，这些数据通常只有在常用的语言资源中有，同时很难将已有的数据转换成其他语言。已有的方法有：利用机器翻译系统简历跨语言分类模型以学习多语言的词向量。
最近，预训练即使不使用任何语料库，也能在小样本跨语言分类任务上表现很好，不过，由于模型参数数量固定，如果增加语言进行训练，丰富资源的语言模型的特征容量就会下降，从而导致多语言模型在下游任务中表现的比通常当语言模型差（翻译不是很准确）。
那么，如何用单语言模型去改进多语言模型呢？

本文基于师生关系提出了多语言精细教学方法(multilingual finetuning method, MONOX). 
