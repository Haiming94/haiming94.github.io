## 【阅读论文】Can Monolingual Pretrained Models Help Cross-Lingual Classification?

- 摘要

因为模型“容量”有限(the constant model capacity)，多语言预训练模型通常落后于单个语言。
文章提出了两种改进小样本(zero-shot)跨语言分类的方法，将当语言训练的模型转移给多语言。
两种方法在跨语言分类的基准任务上实验，比普通的多语言微调效果有效。

- introduction

